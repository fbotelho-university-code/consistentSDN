%!TEX root = ../PEI.tex
\label{sec:feasibility:apps}
\glsresetall


In order to evaluate the feasibility of our design 
we implemented a prototype of the described distributed controller
architecture integrating applications from the Floodlight
controller~\footnote{\url{http://www.projectfloodlight.org/floodlight/}} with a
data store built using a state-of-the-art state machine replication
library, BFT-SMaRt~\cite{smart-tr,bft-smart:2011:High-perfomance}.

In this section we explain how this applications work, the
modifications done in the integration process and an analysis of the
data store performance. To evaluate the feasibility of our design we considered three SDN applications provided with Floodlight: \emph{Learning Switch}  (a common layer 2 switch), \emph{Load Balancer} (a round-robin load balancer) and \emph{Device Manager} (an application that tracks devices as they move around a network).
The applications were slightly modified but 
we made an effort to avoid behavioral changes to the applications.
The main change was shifting state from the controller's (volatile) memory to the data store efficiently (i.e., always trying to minimize communication).
To the best of our knowledge the exposed services of
the applications we changed are virtually indistinguishable from their
predecessors. Changes to the behaviour will be justified in this
chapter. 



The objective of the experiments covered in this chapter  is to analyze the workloads generated
by these applications to thereafter measure the performance of the
data store when subject to such realistic demand caused by real applications.

For this, we need to understand how the applications will use the data
store. We do this by using the workloads of the
applications. Workloads are a digital footprint of the usage of the
data store.  They are fundamental to understand: how the applications
relates with the data store,the performance impact in the data store
and what kind of changes in the controller-to-data store interaction will have more impact in
the performance. We found them also to be a great way to understand
the applications behaviour. 

In figure \ref{fig:feasibility:workloads} we see 

In this chapter we will present the workloads of the three aplications
modified. We will also reveal the iterative process that we have
followed throughout our work while adapting the applications to the
data store functionalities presented earlier (see section
\ref{sec:heimdalldatastore}). In fact, those functionalities only
exist, as a consequence of the original workload observed. On each
iteration we observed behavioural patterns, that we believed to be far
from optimal, and the performed another iteration of the data store
interface development in order to address those issues. We finalized
with another modification to the data store. (and so on...). 

We believe this iterative process to be valuable, since it helps
understanding what kind of bottlenecks and patterns are far from
optimal when adapting existent applications to our design. They can be
useful to understand and improve on the applications. 
 % tradeoffs if we have the code changes. 


The study was done in three phases.  
First, we emulated a network environment in Mininet  --- a network
emulation platform that enables a virtual network, running real
kernel,switch and application code, on a single machine \cite{Handigol:2012t} ---  that consisted of a single switch and at
least a pair of host devices. Then ICMP requests (pings) were
generated between pairs of host devices. The objective was to create OpenFlow  traffic (\texttt{packet-in} messages) from the ingress switch to the controller.
Then, for each OpenFlow (OF) request, the controller performs a
variable, application-dependent number of read and write operations,
of different sizes, in the data store (i.e., the
\textit{workload}). In the controller  (the data store client) we
record each data store interaction entirely (i.e., request and reply
size, type of operation, etc.,)  associated with the data plane event
that has caused it. 

Second, the collected workload traces were used to measure the performance of our distributed data store.
For the experiments we used four machines, three for the distributed
data store\footnote{To tolerate the crash from a single controller
  ($f=1$) three replicas are needed, as explained in Section
  \ref{architecture}.} and one to simulate the data store client (the
controller).  In this phase the objective is evaluate the performance
of the middleware that composes the true bottleneck of our datastore:
bft-smart. Our data store is not designed for performance, and it does
make any sense to include it in the process. So we do not actually use our data store
implementation. Instead, the data store client  replays a simulation of the recorded
workload with equals payloads (i.e., equal message type and payload
size) as well as an additional 4 byte field representing the expected
reply size of the payload. The data store in place, replies with an
message accordingly. From this experience we obtain the throughput and
latency of processing a workload (which corresponds to a single data
plane event). As such our metrics will be flows and time.  We repeat
this experience for a variable of number of concurrent clients as data
store clients. This is required in order to understand the point of
inflexion, ideal, following the queue theory. In practice this clients
can represent different threads in one controller and/or different
controllers using the shared data store. 

In the third phase, we analyze the results of both the first and
second phase. And rationalized on ways to improve them. If we found
one, we modify the data store, application , and then repeat the
process all over again. 




The traces (i.e., data plane events and respective workloads) for each workload seen in this chapter is available online (\cite{support} appendix
A). In there the reader can find: a script automating the data plane
events in Mininet that trigger the OpenFlow messages as well as a recording of
both the data plane events and data store messages characteristics
(i.e., workloads). In there the reader can obtain instruction on how
to use the existent code base to replay all the experiences. 


Each workload (with all its composing operations)  was run 50 thousand
times, measuring both latency and throughput. We calculated averages,
minimum, maximum and standard deviations in the 90, 95 and 99th percentile. Unless stated
otherwise the values shown in this chapter are in the 90th
percentile. Appendix B \cite{support}  contains all this information
in graphical and raw format (as well as the captured workload in
textual information). 

For completeness we should clarify from the outset that 
we do not log creation, deletion of tables, since in our experiences those are not relevant  these happen very
infrequently (e.g., only on the first switch message received by an
application). 


\subsubsection{Development Environment characteristics}
\begin{itemize}
\item TODO - set bullets, not footnotes. 
\item[Quinta hardware] Each machine 
\item Versions of Mininet (mininet problem pointer) \footnote{Version
    2.0 (mininet-2.0.0-113012-amd64-ovf) available at
    \url{http://mininet.org}}. We had an issue with this version that
  caused hosts to offer gratuitous traffic that interfered with our
  experiences. We solved by disabling \gls{ipv6} in the virtual
  machine used to run Mininet\footnote{This is a known problem
      http://goo.gl/DQ7FQF
      (contains the solution followed)}. 
\item Floodlight is available online \footnote{\url{http://goo.gl/RbBXag} commit 9b361fbb3f84629b98d99adc108cddffc606521f}. 
\item Version of smart
  \footnote{\url{http://code.google.com/p/bft-smart} revision 335} 
\item Languages runtimes? 
\end{itemize}

\section{Learning Switch}
\label{sec:feasibility:ls}
\glsresetall
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[ht]

  \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{pic/feasibility/ls-events-broadcast}
                \caption{Broadcast packet.}
                \label{fig:ls:interaction:broadcast}
        \end{subfigure}%
        ~
        \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{pic/feasibility/ls-events-unicast}
                \caption{Unicast packet.}
                \label{fig:ls:interaction:unicast}
        \end{subfigure}
        \caption[Learning Switch workloads]{Broadcast packets trigger a write for the source address of the respective packet. Unicast packets have to additionally read the source address port location.}
        \label{fig:ls:interaction}
\end{figure}

The Learning Switch application emulates the hardware layer 2 switch
forwarding process. 
For each switch a different \emph{\texttt{MAC}-to-switch-port}
table is maintained in the data store. Each table is populated using the source address information (i.e.,
\gls{mac} and switch port)  presented in every OpenFlow \texttt{packet-in}
request for the purpose of maintaining the location of devices. 
After learning this location, the controller can install
rules in the switches to forward packets from a source to a
destination. Until then, the controller must instruct the switch to
\emph{flood} the packet to every port, with the exception of
the ingress port (where the packet came in from). Despite being a
single-reader, single writer application since each switch-table is
only accessed by the controller managing the switch in question. 
we justify the study  of this application for two
reasons: (i) it benefits from the fault-tolerance property of
our distribution and (ii) it is commonly used as the
single-controller benchmark application in the literature
\cite{Tootoonchian:2012uia,beacon-hotsdn}. 


Figure \ref{fig:ls:interaction}  shows the detailed interaction between the
switch, controller (Learning Switch) and data store for two possible
cases. First (figure \ref{fig:ls:interaction:broadcast}), the case for broadcast packets that require
one write operation to store the switch-port  of the
source address. Second (figure \ref{fig:ls:interaction:unicast}),   the case for unicast
packets, that not only stores source information, but also reads the
switch egress port for the destination address.  

In its original state this application maintained an hash table
associating a switch to another hash table relating  \gls{mac} and
\gls{vlan} to switch ports. Both this hash tables were thread-safe
(i.e., supported concurrent manipulation safely). This fits naturally
in our key-value data store client implementation. The smart reader 
will wonder why does the original application is single-reader, single
writer. This happens since the internal state is actually exposed
through the northbound \gls{api} existent.  

Also, it is critical that the switch table is limited due to resource
exhaustion. Remember that each table can potentially keep an entry for
each host present in the network. For this reason the application
limits each table to a fixed number of hosts (1K by default). Whenever
this number is reached, newly added devices replace existent
devices in the table. The eviction policy used, actually uses a
\gls{lru} eviction policy where inactives entries are the first to be
evicted. On each access to the table (get and write, even if the entry
already exists), the entry associated moves to the top of a list,
making it the most recently used. Then on eviction, we only have to replace the
bottom entry by a new one. We mimicked this behaviour in the data
store but in section \ref{sec:feasibility:lsw:cache} ahead, we explain
how we affected it. 

% way better: 
%The LRU discards the least recently used items first. For this,
%whenever an entry is accessed, it moves to the top of the
%list. Whenever and entry is added to the table, but the  capacity is
%in the limit, the last entry in the access list is removed, to give room to the new one.
But the \gls{lru} is not the only way to control the entries present
in each switch table. Learning Switch also applies timeouts (hard and
soft --- see section \ref{section:background:of})  to the flows
installed in the data plane. When they expire, a switch triggers an
\gls{of} \texttt{FLOW\_REMOVED} message (containing a source and a
destination address)to the control plane which, in
turn, deletes the associated entry from the data store and instructs
the switch to remove the reverse entry (from destination to source
address) from its table. This process can then be repeated one more
time. 

\subsection{Broadcast Packet}
Figure \ref{fig:ls:interaction:broadcast}
This workload corresponds to the operations performed in the data
store when processing broadcast packets in a \acrfull{of} 
\texttt{packet-in} request.  Table \ref{table:lsw0:broadcast} shows that for the
purpose of associating the source address of the packet to the ingress
switch-port where it was received, the Learning Switch application performs one
write operation with a request size of 113 bytes and reply size of 1
byte. 

\begin{table}[ht]
\centering 
\begin{tabular}{l c c c c}
 Operation & Type & Request & Reply \\ \toprule 
 Associate source address to ingress port & W & 113 & 1 \\ \bottomrule
\end{tabular}
\caption[Workload lsw-0-broadcast( Broadcast Packet) operations]{Workload lsw-0-broadcast( Broadcast Packet) operations and sizes (in bytes).}
\label{table:lsw0:broadcast}
\end{table}

\subsection{Unicast Packet}
Figure                \ref{fig:ls:interaction:unicast}
Workload \textbf{lsw1-1} is the result of processing an \acrfull{of} request
triggered by an unicast packet. This workload builds on the previous
one, showing an additional operation to query the switch-port location of the
destination address. Table \ref{table:lsw0:unicast} provides a summary all the data
store operations in this workload. 

\begin{table}[ht]
\centering 
\begin{tabular}{l c c c c}
 Operation & Type & Request & Reply  \\ \toprule 
 Associate source address to ingress port & W & 113 & 1\\\midrule
Read egress port for destination address & R & 36 & 77 \\\bottomrule
\end{tabular}
\caption[Workload lsw-0-unicast( Unicast Packet) operations]{Workload lsw-0-unicast( Unicast Packet) operations and sizes (in bytes).}
\label{table:lsw0:unicast}
\end{table}

\subsection{Optimizations}
The Learning Switch workloads are very simple. So there is
not much we can do to improve them. Still, we noticed that there were
some extreme overhead to the messages sizes given the content that is
actually exchanged between the data store: a MAC address (8 byte
standard); and  a switch port identifier. The reason is simple, we
were the standard Java serialization process to
transform objects into byte arrays (used in the data store), and
vice-versa. This incurs in some standard overhead required by the Java
process. If we do it manually we can improve a lot on the size of the
messages exchanged as seen in table
\ref{table:lsw1:unicast}. Considering the total size of the messages
(request $+$ reply) we have actually reduced the equivalent unicast workload (table
\ref{table:lsw0:unicast} by 72\%. The same goes for the broadcast,
workload (the first message of table \ref{table:lsw-1-unicast}. 

Figure \ref{fig:lsw:comparison}  shows the results of the performance
analysis made to the four workloads. Each workload  curve has different points
taken from tests with increasingly client numbers. Accurate values and standard
deviations, as well as measures in the 95th and 99th percentile, can
be seen online \ref{support} (appendix ?).  It was somewhat surprising
that the difference in the performance between the original versions
(prefixed by lsw-0) and the optimized size versions (prefixed by
lsw-1) is unnoticeable (in some cases lsw-1 is worst than lsw-0 due to
the statistical variance). It can be that the network packet exchanged by
client and the data store (or between the data store replicas) is
actually not affected (in size) by this change. However, we will soon
verify that size optimizations are bearably unnoticeable in all our examples, except
when differing in order of magnitude. 

But we can see a significant difference between unicast and broadcast
workloads due to the fact that the former requires one more message
than the former. For the broadcast workload we can support 20kFlows/s
with a 3 ms latency. For the broadcast workload we have 12kFlows/s
with the same 3ms latency. 

The natural conclusion we can take, is to think that if we merge the
two messages that compose the broadcast workload into one (by using
Micro Components - see section \ref{sec:heimdall:datastore:mc}) we
should obtain performance results equivalent to the broadcast
workload. This is true, considering that if we add up the
lsw-1-unicast message sizes we get a very similar workload to
lsw-1-broadcast. However we do not do this, since that with cache, we
can potentially obtain much better results without requiring this. 

\begin{table}[ht]
\centering 
\begin{tabular}{l c c c c}
 Operation & Type & Request & Reply \\ \toprule 
Associate source address to ingress port & W & 29 & 1\\\midrule
Read egress port for destination address & R & 27 & 6 \\\bottomrule
\end{tabular}
\caption[Workload lsw-1-unicast( Unicast Packet) operations]{Workload lsw-1-unicast( Unicast Packet) operations and sizes (in bytes).}
\label{table:lsw1:unicast}
\end{table}

\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{../data/reportGenerator/lsw-0-broadcastlsw-0-unicastlsw-1-broadcastlsw-1-unicasttxLatCmp.pdf}
\caption[Learning Switch workloads performance comparison]{Learning
  Switch workloads performance comparison (90th percentile). }
\label{fig:lsw:comparison}
\end{figure}


\subsection{Cache}
\label{sec.learning.switch.lru.cache} 
\label{sec:feasibility:ls:cache}

Given that Learning Switch is a single reader, single writer
application, we can introduce caching mechanisms without impairing the
consistency semantics. To all effects, the Learning Switch can contain
an exact copy of everything that is present in the data store at every time.

As so we can completely avoid the data store as long as we have
entries in cache.
First we can avoid re-writing the source address to source port association when we already now it.
in the original Learning Switch this re-write is not costly
($\Omega(1)$) and has the functional impact of refreshing the entry
timestamp such that the least recently used table can keep up
consistently with the last active host and delete the inactive ones
(that may have moved or disconnect). But with the data store this is
much more expensive. 
Now, the active host
actually gets forgotten somewhere in time as newly (unknown) entries
are added to the data store, we expect this to be ok since the host,
being active, will benefit in latency a lot before actually being
erased from the data store due to the newly added hosts. 
When avoiding this write in the cache implementation we must actually be sure that we only
avoid to write to the data store when the association is known in
cache and it is actually correct (the ingress port is the same from
the packet being processed). 
The second avoidance is the read operation that queries for the egress
port of the current processed packet. We do not actually need to read
from the data store if the entry is present in the cache. First the
data is not modified by any other controller since we are the only
ones which manipulate our switch tables. 

With this improvement we no longer have to read values from the
database. We do not need since when we update the data store  (with
writes) we also update the cache. So if it isn't on the cache it is not in the
data store. This means that we can avoid our cache timed interface
(see section \ref{sec:heimdall:cache}) and just read from the data
store every time. Off course this means that the cache must be able to
handle the same state size that the data store (which may be
infeasible). 

The reader may think of an exception where we find stale data:
when a host moves from a switch to another, the tables from the first
switch will have  incorrect data and devices will be unreachable from
that switch from some time. But this also happens with the centralized
version also. This is why rules installed in the switches must have a
idle and hard timeout set. When one of the timeouts expire the switch
triggers an \texttt{FLOW\_REMOVED} message to the controller, that in
turns deletes the respective information in the data store.  This kind
of problems reside on the data plane consistency side. Not on the
control plane. 




Cache can only improve on the overall analysis of the overall system
(controller and data store) since the cache logic actually resides on the
controller and not the data store.

\todo{Finish up with reference or explanation of why we did not do it}

%We don't improve on the workload.  
%We don't actually improve on the micro-benchmarks tested measures
%shown throughout this chapters. We do not improve simply because with
%cache we do not avoid or improve (by size reduction) any of the data
%store interactions present in table \ref{table:work:lsw1-1} (that
%shows the latest learning switch workload).  With cache we will only
%improve on the long run, since we can now avoid the two type of
%requests present in that table.


\section{Load Balancer}
\label{sec:feasibility:lb}
\glsresetall
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Load Balancer main idea} 
The Load Balancer application employs a round-robin algorithm to distribute the
requests addressed to a \gls{vip} . 
In order to understand its behaviour we will begin by the data model currently used. Figure
\ref{fig:lb-model} shows the three different entities used in the Load
Balancer. The \gls{vip} entity represents a virtual endpoint with a specified \gls{ip}, port and
protocol address. Each \gls{vip} can be associated with one or more pools of 
servers. Given that the distribution algorithm is round-robin, each pool
has a current assigned server (\texttt{current-member} attribute in the figure). Finally, the third entity --- Member
--- represents a real server. 
\item Each of those entities, corresponds
to a different  table  in the data store, indexed by the entity
key attributes represented in the figure (in bold). Moreover, a fourth
table, named \texttt{vip-ip-to-id} is required to associate \gls{ip} addresses to \gls{VIP} resources. 



\begin{figure}[ht]
\TopFloatBoxes
\begin{floatrow}
\ffigbox{
\includegraphics[scale=0.6]{./pic/feasibility/lb-model.pdf}
}{\caption{\small Simplified Load Balancer entities data model. The data
store contains a table for each entity, indexed by their keys (represent as bold attributes). }
\label{fig:lb-model}}


\capbtabbox{
\small
\begin{tabular}{cccc}
  Name & Key & Value & \\ \toprule
vips  & vip-id  & vip   \\\midrule
pools & pool-id &  pool \\\midrule
members & member-id  & member    \\\midrule
vip-ip-to-id &  ip & vip-id   \\\midrule
\end{tabular}
}{\caption[Load Balancer key-value tables]{Load Balancer key-value tables.}
\label{tablle:lb:indexes}}
\end{floatrow}
\end{figure}

In light of this data model, the load balancer logic requires the following
operations from the data store: (i) check if the source address is
associated with a VIP resource; (ii) if so, read the VIP, Pool and
Member information required to install flows in the switch and (iii)
update the pool \texttt{current-member} attribute. This description
corresponds to the case where \gls{of} \texttt{packet-in} requests are indeed addressed at a \gls{vip}
resource. The respective workload which, is the heavier in
the Load Balancer application, is represented in Fig. \ref{fig:lb:interaction:ip2Vip}. Alternatively, Fig. 
\ref{fig:lb:interaction:arp2VIp}  considers the special case of ARP requests questioning the hardware
address of a \gls{vip} \texttt{IP}. In the following sections we
expand on the details and improvements related to both those
workloads. 

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{pic/feasibility/lb-events-broadcast}
                \caption{ARP packet address at a VIP.}
                \label{fig:lb:interaction:arp2Vip}
        \end{subfigure}%
        ~
        \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{pic/feasibility/lb-events-unicast}
                \caption{IP packets addressed at a VIP. }
                \label{fig:lb:interaction:ip2Vip}
        \end{subfigure}
        \caption[Load Balancer workload events]{A \texttt{\gls{arp}} request message addressed at a VIP \gls{ip} that results in a direct \gls{arp} reply. On the left a normal \gls{ip} packet addressed at VIP should be resolved (who is responsible) and replied by installing the appropriate rules}  
        \label{fig:lb:interaction}
\end{figure}

\subsection{Packets to a VIP}
When the Load Balancer receives a data packet addressed
at a \gls{vip}, it triggers the operations seen in table \ref{table:lbw-0-ip-to-vip}. 
The first two operations fetch a \gls{vip} resource associated with the
destination \gls{ip} address of the packet. The first fetches the
\gls{vip} unique identifier associated with the destination
\gls{ip}. If it succeeds, the reply is different from 0 and it can
proceed to the second operation where the \gls{VIP} entity is fetched
from the data store. 
Following it, the third operation fetches the chosen pool for the returned  \gls{vip} \footnote{The current implementation of this
application always chooses the first existent pool. This behaviour
exists because the original developers anticipated a more robust Load Balancer application
where different pools can be associated with a \gls{VIP} to enhance
the balancing algorithm with feedback of the application servers or
possibly the network state.}.

Afterwards it updates the fetched  pool with the newly modified
\texttt{current-member}. The forth and final operation retrieves
the address information for the selected information. 

\begin{table}[ht]
\small
\centering 
\begin{tabular}{l c c c c}
 Operation & Type & Request & Reply \\ \toprule 
Get VIP id for the destination IP & R & 104 & 8\\
Get VIP Info (pool information) & R & 29 & 509\\
Get the chosen pool & R & 30 & 369\\
Conditional replace pool after round-robin & W & 772 & 1\\
Read the chosen Member & R & 32 & 221 \\
\end{tabular}\caption[Workload lbw-0-ip-to-vip( IP packet to a VIP)
operations]{Workload lbw-0-ip-to-vip( IP packet to a VIP) operations
  and sizes (in bytes).}
\label{table:lbw-0-ip-to-vip}
\end{table}



\subsection{ARP Request}
This workload  results  from processing an ARP Request addressed at a
\gls{vip} address. The data store operations, summarized in Table
\ref{table:lbw-0-arp-request}, shows that two reads are
required. First, as previously seen,  it queries the data
store to check if the packet destination address is a \gls{vip} (1 read
needed). As it is, the controller then retrieves the \texttt{MAC} address for that
\gls{vip} server (so, another read is needed to obtain the \gls{vip} entity).

\begin{table}[ht]
\small
\centering 
\begin{tabular}{l c c c c}
Operation & Type & Request & Reply \\ \toprule 
Get VIP id for the destination IP  & R & 104 & 8\\
Get VIP info (proxy MAC address) & R & 29 & 509 \\\bottomrule
\end{tabular}\caption[Workload lbw-0-arp-request( Arp Request to a
VIP) operations]{Workload lbw-0-arp-request( Arp Request to a VIP)
 operations and sizes (in bytes).}
\label{table:lbw-0-arp-request}
\end{table}

\subsection{Optimizations}
\begin{table}[ht]
\small
\begin{tabular}{llccccc}
 Operation & Type &  \multicolumn{5}{c}{ (Request, Reply) } \\  \midrule
&  & lbw-0 & lbw-1  & lbw-2 & lbw-3 & lbw-4 \\ \toprule 
%& &   \multicolumn{5}{c}{(Request, Reply)} \\midrule 
Get VIP id of destination IP  & R & (104,8) &\multirow{2}{*}{(104,509)} &  \multirow{2}{*}{(104,513)} &\multirow{2}{*}{\textbf{(62,324)}} & \multirow{2}{*}{-}    \\\cmidrule{1-2} 
Get VIP info (pool)   & R &  (29,509) & & & &   \\ \midrule 
Get the choosen pool  & R & (30,369)  &  - & (30,373) & -   & \multirow{3}{*}[-2mm]{\textbf{(11,4)}}  \\  \cmidrule{1-2} 
Replace pool after round-robin  & W & (772,1) & -
&\textbf{(403, 1)} &  - \\ \cmidrule{1-2}  
  Read the chosen Member &  R & (32,221) & - & (32,225) & \textbf{(44,4)} & \\\bottomrule  
\end{tabular}\caption[Load Balancer IP to VIP workload operations across
diferent implementations.]{Load Balancer  lbw-\textit{X}-ip-to-vip workload
  operations and respective sizes (in bytes) across diferent
  implementations. Bolded sizes represent significant differences
  across implementations. Sizes marked with \texttt{-} are equal to
  the previous. } 
\end{table}

\begin{figure}[ht]
% \CenterFloatBoxes
%\TopFloatBoxes  
% \BottomFloatBoxes
\begin{floatrow}
\ffigbox{%
  \includegraphics[scale=0.4]{../data/reportGenerator/lbw-0-ip-to-viplbw-1-ip-to-viplbw-2-ip-to-viplbw-3-ip-to-viplbw-4-ip-to-viptxLatCmp.pdf}
}{\caption{Cenas}%
}
\capbtabbox{%
\small
  \begin{tabular}{lll} 
    Prefix &  Data store & Section\\\toprule
    lbw-0 & Simple Key-Value  & \ref{sec:}  \\
    lbw-1 & Cross References  & \ref{sec:} \\
    lbw-2 & Versioned Values & \ref{sec:} \\
    lbw-3 & Column Store & \ref{sec:} \\
    lbw-4 & Micro Components & \ref{sec:} \\ \bottomrule
    & &  \\ 
    & &  \\ 
    & &  \\ 
    & &  \\ 
    & &  \\ 
  \end{tabular}
}{%
  \caption[Name guide to Load Balancer workloads]{Name guide to Load
    Balancer workloads.}\label{table:lb-versions}
}
\end{floatrow}
\end{figure}

Table \ref{table} shows us the all the modifications we have done to
the workload triggered by a packet addressed at a \gls{VIP}
address. It is somewhat similar to previous workload tables (e.g,
\ref{tables}), but this time, we show the workload according to
different implementations of the application using a variety of data
store functionalities. This way we can verify the impact of each
functionality individually. For reference we prefix the workload name
with a different name: lbw-0, lbw-1, ..., lbw-4. The functionality
used with that prefix can be seen in table
\ref{table:lb-versions}. The prefix lbw-0 refers to the simple
key-value store implementation which has already been presented in
table \ref{\ref{table:lbw-0-ip-to-vip} that summarized  the workload 
operations. Message size is now grouped in tuples. When no difference is noticed between implementations we use the \texttt{-}
symbol instead of a tuple. Significant changes are emphasized in
bold. Also notice that some operations are merged together (first
two operations on workload lbw-1 and the last three in workload
lbw-4.\\ 

For the first improvement we eliminate the double step required to
obtain a \gls{vip} in the first two messages. This can be done with
the Cross Reference functionality by configuring the data store with
the information that each value of the \texttt{vip-ip-to-id} table
(consulted in the first workload operation) is actually a key for
the \gls{vip} table. Then the data store, can fetch the values from 
 HERE
\item \textbf{Cross Reference} (lbw-1) 
\item merge messages. No need to get the an id in the first
  message and then the vip (obtained with the first id) 
\item  So both arp request and the packet address at a VIp workloads show
requests that first obtain the VIP id and only aftewards read the VIP
from the VIP table using the obtained id. So we can improve our
workload footprint by directly obtaining the VIP with only the first
request. 
\end{itemize}

\begin{itemize}
\item \textbf{Versioned Values} (lbw-0)
\item Cut size in half. 
\item The conditional replace (after round-robin) in the 4th line is
  requires sending two pools: the original one fetched and the
  modified one.  Then only if the original pool matches the one
  present in the data store, the new pool will be inserted. If not the
  operation fails. 
\item By using version numbers associated with each entry in the data
  store, we could potentially replace the values. 
\item Notice that read operations now have an increased 4 bytes for
  the version number. 
\end{itemize}

\begin{itemize}
\item \textbf{Columns}
\item We tried to optimize by using columns. But not much success
\item Obtain the pool actually requires read a lot of he fields of the
  original value for later processing. So the size reduction is not
  significant. 
\item Obtaining the member is actually a good increase. The reply
  size is now 56 times lower than before. 
\end{itemize}

\begin{itemize}
\item \textbf{Micro Components}
\item We can't avoid going to the data store for fetching and updating
  the pool, for the round robin algorithm
\item A form of optimistic concurrency control that will certainly
  fail a lot if different controllers manipulate the same pools. (so
  it would be beneficial, if by design, pools were partitioned across
  controllers)
\item So we can merge the three operations in a \texttt{round-robin}
  method that returns the required member information. 
\end{itemize}
\end{itemize}

\subsubsection{Analysis}
\begin{itemize}
\item We see the same pattern again. 
\item Size reduction in the same order of magnitude, do not have any
  practical impact.
\item Message reduction has a great impact. 
\item Minor improvement from 0 ( 4.5k/s,  4 ms) to  1 (6.1k , 6.7) ms,
  caused by message reduction. 
\item Message size reduction across the three workloads: 1,2 and 3
  does not have a deep impact. As can be see in the graphic it is not
  enough to differentiate the lines. 
\item  But message reduction, in the last workload (4) , with
  Micro-Components, pays off a lot. 12kFlows/s with 5 ms latency. An
  improvement of more than double from the original workload, with
  only a 25\% increase in latency. 
\end{itemize} 


In the ARP request case  we improve by using cross references tables
to join the two requests into one (lbw-1-arp-request). With the column
based implementation we improve slightly by reducing the size of the
requests as I see.

\subsection{Cache}

\subsubsection{What to cache. Effects of cache}

\begin{itemize}
\item Considering the ip-to-a-vip workload
\item Fetching the vips  can be cached. 

\item VIP information can be invalid. We try to reach a VIP that does not
  exist anymore for some reason, or may have changed IP address to
  start providing another service. This can happen already in the
  strongly consistent version. When we fetch a VIP from the data
  store, it can already be invalid, by the time we get it
  locally. With cache the probability is greater.  

\item Updating the timestamps can not be cached. 
\end{itemize}

\subsubsection{The reason columns are so big}
\begin{itemize}
\item We fetch \gls{vip} entities in two different workloads: arp
  requests, ip to VIP. 
\item fetch the union of the information required for the two
operations.
\item may be beneficial if the flows are dependent of one another
\item  When processing the first we can be sure that unless some
anomaly happens the hosts performing the arp request will consequently
trigger the second flow processing.
\item So we benefit from having the
information in cache to avoid having to perform another request just
because a field is missing. 
\end{itemize}

\subsubsection{Why cache can be so important}



\begin{itemize}
\item Minimum footprint of this application in the overall controller
  pipeline. 
\item Even when processing a normal packet, not related to a VIP address at
all, the Load Balancer still has to find out if this is the case. This
workload, which only requires one operation (see table (First line of tables))
\ref{table:lbw-0-ip-to-vip,table:lbw-0-arp-request} but with  a 0 byte
reply) sets the minimum amount of work imposed by
the Load Balancer to the controller pipeline. 
\item Empty values. 

%Ideally we should also avoid the normal case of IP packets not
%addressed at a VIP. For this our cache  must understand what a empty
%value means FIXME. (use containsInCache . update to insert empty in
%cache. Then see if containsInCache AND get == null you can be certain
%the value is not a VIP), completely avoiding the going to the data store.
\end{itemize}



\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{./../data/reportGenerator//lbw-3-ip-to-notviptxLat.pdf}
\caption[Minimum impact of Load Balancer in the pipeline.]{Workload
  lbw-3-ip-to-notvip shows the minimal impact the Load Balancer
  applications has on the pipeline in our best implementation.}
\end{figure}


\subsubsection{Cache}

\section{Device Manager}
\label{sec:feasibility:dm}
\glsresetall
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


The Device Manager application tracks and stores host devices
information such as the switch-ports to where devices are
attached to\footnote{The original application is able to track devices as
  they move around a network, however our current implementation does
  not take this feature into consideration.}. This information ---
that is retrieved from the OpenFlow packets that the controller receives --- is crucial to
Floodlightâ€™s Forwarding application. That is to say, that for  each new flow, the Device
Manager has the job of finding a switch-port for both the destination
and source address. Given this information, it is able to pass it to
the Forwarding application, that can later decide on the actions to
take (e.g., best route). Notice that this arrangement, excludes the
Learning Switch as the  forwarding application in action. 

Regarding the data store usage, Device Manager requires three
data store tables listed in table \ref{table:dm:indexes}.  The first
table, \texttt{devices} keeps track of known devices created by the
application. A second table named \texttt{macs}  indexes those same devices by their
\gls{mac} and \gls{vlan}  pair.  Finally, a third table named
\texttt{ips} maintains an index from an \gls{ip} address to one or
more devices.


\begin{table}
\small
\begin{tabular}{cccc}
Name & Key & Value & \\ \toprule
devices & device-id &  device \\\midrule
macs & (MAC,VLAN)  & device-id   \\\midrule
ips  & IP & device-id list \\\midrule
\end{tabular}
\caption[Device Manager key-value tables]{Device Manager key-value tables.}
\label{table:dm:indexes}
\end{table}

We will analyze and improve on  two distinct workloads for this application differing in
wether the application already knows the source device information (
figure \ref{fig:dm:interaction:known})
or not ( \ref{fig:dm:interaction:unknown}). 

In the former case, the
application mainly reads information from the data store in order to
obtain location information. As for the latter case, the
application must create the device information and updates all the
existent tables. Therefore, this workload generates more traffic between
the controller and data store. 


\begin{figure}
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{pic/feasibility/dm-unknown}
                \caption{Packet from an unknown device.}
                \label{fig:dm:interaction:unknown}
        \end{subfigure}%
        ~
        \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{pic/feasibility/ls-events-unicast}
                \caption{Packet from a known device.}
                \label{fig:dm:interaction:known}
        \end{subfigure}
        \caption[Device Manager workload events]{Workloads for this application heavily depend on the state of the data store. Unknown devices trigger several operations to the creation of these, while known devices only require an update of their "last seen" timestamp. No matter the case, the source and destination devices are retrieved if they exist.}
        \label{fig:dm:interaction}
\end{figure}

\subsection{Known Devices}

\begin{table}[ht]
\small
\centering 
\begin{tabular}{l c c c c}
Operation & Type & Request & Reply \\ \toprule 
Read the source device key & R & 408 & 8\\
Read the source device & R & 26 & 1444\\
Update "last seen" timestamp & W & 2942 & 0\\
Read the destination device key & R & 408 & 8\\
Read the destination device & R & 26 & 1369 \\
\end{tabular}
\caption[Workload dm-0-known (Known Devices) operations]{Workload
  dm-0-known (Known Devices) operations and sizes (in bytes).}
\label{table:ops:dm-0-known}
\end{table}

When devices are known to the application, a \texttt{packet-in} request
triggers the operations seen in table \ref{table:ops:dm-0-known}. The
first two operations read source and destination device
information in order to make their switch-ports available to the
Forwarding process. Additionally, the second operation (a write), 
updates the ``last seen'' timestamp of the source device.

\subsection{Unknown Source}
\small
\begin{table}[ht]
\centering 
\begin{tabular}{l c c c c}
Operation & Type & Request & Reply \\ \toprule 
1) Read the source device key & R & 408 & 0\\
2) Get and increment the device id counter & W & 21 & 4\\
3) Put new device in device table & W & 1395 & 1\\
4) Put new device in \texttt{(MAC,VLAN)} table & W & 416 & 0\\
5) Get devices with source IP & R & 386 & 0\\
6 ) Update devices with source IP & W & 517 & 0\\
7) Read the destination device key & R & 408 & 8\\
8) Read the destination device & R & 26 & 1378 \\\bottomrule
\end{tabular}
\caption[Workload dm-0-unknown( ARP from Unknown Source)
operations]{Workload dm-0-unknown( ARP from Unknown Source) operations
  and sizes (in bytes).}
\label{table:ops:dm-0-unknown}
\end{table}


This workload is triggered in the specific case in which  the source device
is unknown and the \gls{of} message carries an \gls{arp} reply 
packet. Seing that both these  conditions are true, the application
proceeds  with 8 data store operations, described in table
\ref{table:ops:dm-0-unknown}. Their intention is to create device
information and update the three tables described  in the beginning
of this section.  

The first operation reads the  source device key. Being
that it is not known, this operation fails (notice in the table, that
the reply has a size  of zero bytes). As a result the application
proceeds with the creation of a device. For this, the
following write (second operation) atomically retrieves
and increments a device unique \texttt{id} counter. Afterwards, the third and fourth  operation
update, with the newly created device, the device and MAC/VLAN
tables respectively. Likewise, the fifth and sixth operations update
the \gls{ip} index table. Given that this index links an \gls{ip} to
several devices we are forced to first collect the set of devices in
order to update it. This \emph{read-modify} operation can
fail in case of concurrent updates. Under that case, both operations
would be repeated until they succeed. At this point, the Device Manager
is done with the creation of the device and can, finally, move to the
last two operations to fetch the destination device information. 

\subsection{Optimizations}
\begin{figure}
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{../data/reportGenerator/dm-0-unknowndm-1-unknowndm-2-unknowndm-3-unknowndm-4-unknowntxLatCmp.pdf}
                \caption{}
                \label{fig:}
        \end{subfigure}%
        ~
        \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{../data/reportGenerator/dm-0-knowndm-1-knowndm-2-knowndm-3-knowndm-4-knowntxLatCmp.pdf}
                \caption{}
                \label{}
        \end{subfigure}
        \caption[Device Manager performance analysis]{}
        \label{fig:dm:performance}
\end{figure}

\begin{table}
\small
\begin{tabular}{lll} 
    Prefix &  Data store & Section\\\toprule
    dm-0 & Simple Key-Value  & \ref{sec:heimdall:datastore:kv}  \\
    dm-1 & Cross References  & \ref{sec:heimdall:datastore:cr} \\
    dm-2 & Versioned Values & \ref{sec:heimdall:datastore:vr} \\
    dm-3 & Column Store & \ref{sec:heimdall:datastore:cr} \\
    dm-4 & Micro Components & \ref{sec:heimdall:datastore:mc} \\ 
  \end{tabular}
  \caption[Name guide to Device Manager workloads]{Name guide to
    Device Manager workloads.}
  \label{table:names:dm}
\end{table}

\begin{table}[ht]
\small
\centering
\begin{threeparttable}
\begin{tabular}{ll ccccc}
 Operation & Type &  \multicolumn{5}{c}{ (Request, Reply) } \\  \midrule
&  & lbw-0 & lbw-1  & lbw-2 & lbw-3 & lbw-4 \\ \toprule 
Get source key & R &(408, 8) & \multirow{2}{*}{(408,1274)} &
\multirow{2}{*}{(408,1278)} & \multirow{2}{*}{(486,1261)} &
\multirow{2}{*}{(28,1414)} \tnote{a} \\ \cmidrule{1-2}
Get source device & R & (26,1444) & & & & \\ \midrule
Update timestamp & W & (2942,0) & (2602,0) & (1316,1) & (667,1) & 
(36,0) \\ \cmidrule{1-2}
Get destination key & R & (408,8) & \multirow{2}{*}[-1mm]{(408,1199)} &
\multirow{2}{*}[-1mm]{(408,1203)} & \multirow{2}{*}[-1mm]{(416,474)} &
\multirow{2}{*}[-1mm]{N/A} \\ \cmidrule{1-2}
Get destination device & R & (26,1369) &  &
 & & \\\bottomrule
\end{tabular}
\caption[Workload dm-0-known( Known Devices) operations]{Workload
  dm-0-known( Known Devices) operations and sizes (in bytes).}
\begin{tablenotes}
\item [a)] This operation also fetches the destination device.
\end{tablenotes}
\end{threeparttable}
\end{table}

%TODO - do not use put new device in MAC,VLAN table. This is
%confusing. 

\begin{table}[ht]
\small
\centering 
\begin{threeparttable}
\begin{tabular}{ll ccccc}
 Operation & Type &  \multicolumn{5}{c}{ (Request, Reply) } \\  \midrule
&  & lbw-0 & lbw-1  & lbw-2 & lbw-3 & lbw-4 \\ \toprule 
Read source key & R & (408,0) & - & - & (486,0) & (28,201)\tnote{a}\\
Increment counter & W & (21,4) & -  & - & - & \multirow{5}{*}{(476,8)} \\
Update device table & W & (1395,1) & (1225,1)\tnote{b}  & - &
(1183,1) & \\
Update MAC  table & W & (416,0) & - & - & -
& \\
Get from IP index & R & (386,0) & - & - & - & \\
Update IP index  & W & (517,0) & - & - & - & \\
Get destination key & R & (408,8) &
\multirow{2}{*}{(408,1208)}\tnote{b} & \multirow{2}{*}{(408,1212)} &
\multirow{2}{*}{(416,474)} & \multirow{2}{*}{N/A}  \\ 
Get destination device & R & (26,1378)  &  & & \\\bottomrule
\end{tabular}
\caption[Workload dm-0-unknown( ARP from Unknown Source)
operations]{Workload dm-0-unknown( ARP from Unknown Source) operations
  and sizes (in bytes).}
\begin{tablenotes}
\item [a)] This operation also fetches the destination device.
\item [b)] Differences in sizes caused by a SERIALIZATION improvement 
\end{tablenotes}
\end{threeparttable}
\end{table}


\subsubsection{Cross References tables}
\begin{itemize}
\item So in practice we have to get the device key from the mac/vlan
  table and only then we can actually get the device information we
  want. This is redudant, and we can actually perform the all
  operation instantaneously  by using cross references tables. 

\item 
\end{itemize}



\subsubsection{Timestamps}
\subsection{Cache}

\subsubsection{What to cache}
\begin{itemize}
\item Source device 

\item With cache we fetch known devices in an optimistic concurrency
manner.  If there is no such device in cache, we then try to obtain it
from the data store, as it might be created from other
controller. (Really? - yes same device with different routes that go
through different openflow controllers)

\item What we try to do is to fetch device from the cache. At some point in
time in a  normal network, we hope that all device information is
known. After that point devices in cache should pass the timestamp
update at first (if they are not updated by concurrent
controllers). If the devices are connected to different openflow
islands simultaneously than this is a bad idea since we will actually
have to perform one more request that the normal workload
pattern. (try to updated - fails, retrieve new , update) . Off course
this could be mitigated by having the update attempt to return the
currently present value timestamp..
\end{itemize}




\subsubsection{Columns}


\subsubsection{Micro Components}
Just a proof of concept. Establish special methods in the data store. 
Three : 
createDevice <-
updateDeviceTimestamp <- 
getTwoDevices <- could be done with transactions.  Gets the source
device entirely. Also gets the attachments points of the destination
device since these are required to forwarding. 

\subsubsection{Everything}
\begin{itemize}
\item the question that may arise: why not simply put everything inside the
  data store then? Well, in the case of the device manager, there are
  actually a few subitilidades hidden in our explanation. First there
  is a dependency of others services. topology manager. 
\end{itemize}



%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../PEI"
%%% End: 

